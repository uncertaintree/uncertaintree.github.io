---
title: "Statictical Modeling and Advanced Regression Analyses"
subtitle: "R Tutorials"
date: now
theme:
  - cosmo
  - quartosupport2.scss
highlight-style: monochrome
linkcolor: gray
notebook-links: false
date-format: long
bibliography: references.bib
csl: quartosupport3.csl
author:
  - name: Holger Sennhenn-Reulen\textsuperscript{\orcidlink{0000-0002-4782-4387}} \linebreak \textsuperscript{Northwest German Forest Research Institute (NW-FVA), Germany.}
    orcid: 0000-0002-4782-4387
    corresponding: true
    email: holger dot sennhenn at nw minus fva dot de
    affiliation: "Northwest German Forest Research Institute (NW-FVA)"
    roles:
      - Statistician
      - Researcher
plain-language-summary: |
  This document shows R code and results. 
key-points:
  - kp1
  - kp2
---

<!-- ```{r} -->
<!-- if(knitr::is_latex_output()) { -->
<!--   knitr::opts_chunk$set(dev = 'tikz') -->
<!--   options(tikzDocumentDeclaration = "\\documentclass{article}\n\\usepackage[lining]{ebgaramond}") -->
<!-- } -->
<!-- ``` -->

`r if (knitr::is_latex_output()) '\\clearpage'`

# Software

We use the statistical software environment *R* [@RCoreTeam2024], and R add-on packages *ggplot2* [@Wickham2016].

<!-- and *plyr* [@Wickham2011].  -->
<!-- *colorspace* [@StaufferEtAl2009] -->
<!-- **cowplot** [@Wilke2023] -->

This document is produced using *Quarto* [@AllaireEtAl2024]. 
<!-- Graphics in the PDF version are rendered using R add-on package *tikzDevice* [@SharpsteenBracken2023].  -->

## Organize R Session

```{r}
#| eval: true
#| echo: true
rm(list = ls())
library("ggplot2")
```

# Linear Regression Model

## Data Simulation

Data are simulated according to the equations given in the lecture slides[^footnoteA]:

[^footnoteA]: For two covariates `x_1` and `x_2`.

```{r}
#| eval: true
#| echo: true
set.seed(123)
N <- 500
df <- data.frame(x_1 = runif(n = N), 
                 x_2 = runif(n = N))
(beta_0 <- rnorm(n = 1, mean = 1, sd = .1))
(beta_x_1 <- rnorm(n = 1, mean = 1, sd = .1))
(beta_x_2 <- rnorm(n = 1, mean = -.5, sd = .1))
(sigma <- rgamma(n = 1, shape = 1, rate = 4))
df$mu <- beta_0 + beta_x_1 * df$x_1 + beta_x_2 * df$x_2
df$y <- df$mu + rnorm(n = N, mean = 0, sd = sigma)
```

### Visualisations

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimA
#| fig-cap: "Scatterplot of the two simulated covariates `x_1` and `x_2` - each from the uniform distribution between $0$ and $1$."
ggplot(data = df, aes(x = x_1, y = x_2)) + 
  geom_point()
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimB
#| fig-cap: "Scatterplot of covariate `x_1` with response `y` - each individual observation is coloured according to the second covariate `x_2`."
ggplot(data = df, aes(x = x_1, y = mu, color = x_2)) + 
  geom_point()
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimC
#| fig-cap: "Scatterplot of covariate `x_2` with response `y` - each individual observation is coloured according to the first covariate `x_1`."
ggplot(data = df, aes(x = x_2, y = mu, color = x_1)) + 
  geom_point()
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimD
#| fig-cap: "Scatterplot of the two simulated covariates `x_1` and `x_2` - each individual observation is coloured according to the underlying true conditional expectation `mu`."
ggplot(data = df, aes(x = x_1, y = x_2, color = mu)) + 
  geom_point()
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimE
#| fig-cap: "Scatterplot of the two simulated covariates `x_1` and `x_2` - each individual observation is coloured according to the response `y`."
ggplot(data = df, aes(x = x_1, y = x_2, color = y)) + 
  geom_point()
```

## Modeling

The basic R command for (frequentist) estimation of the parameters of a linear regression model is a call to the function `lm`:

```{r}
m <- lm(y ~ x_1 + x_2, data = df)
summary(m)
```

### Visualisations

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimF
#| fig-cap: "Scatterplot of covariate `x_1` with the true conditional expectation `mu` - each individual observation is coloured according to the second covariate `x_2`. The line gives the point estimation for the conditional expectation with the second covariate `x_2` fixed to $0.5$."
nd <- data.frame(x_1 = seq(0, 1, by = .1), 
                 x_2 = .5)
nd$mu <- predict(m, newdata = nd)
ggplot(data = df, aes(x = x_1, y = mu, color = x_2)) + 
  geom_point() + 
  geom_line(data = nd, aes(x = x_1, y = mu, color = x_2))
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimG
#| fig-cap: "Scatterplot of covariate `x_1` with the true conditional expectation `mu` - each individual observation is coloured according to the second covariate `x_2`. The lines give the point estimation for the conditional expectation with the second covariate `x_2` taking on values between $0$ and $1$ (at steps of $0.1$)."
nd <- data.frame(expand.grid('x_1' = seq(0, 1, by = .1), 
                             'x_2' = seq(0, 1, by = .1)))
nd$mu <- predict(m, newdata = nd)
ggplot(data = df, aes(x = x_1, y = mu, color = x_2)) + 
  geom_point() + 
  geom_line(data = nd, aes(x = x_1, y = mu, color = x_2, group = x_2))
```

## Add-Ons

### Add-On Linear Model: A) Stancode

#### Stan Users Guide

*Probabilistic Programming Languages* such as *Stan* [@CarpenterEtAl2017] allow to `plug together` the single parts of a statistical regression model[^footnoteB]:

[^footnoteB]: Which is actually pretty 'readable' if you get used to the structure for a simple model such the linear regression model. 

The following Stan-code is published [here](https://mc-stan.org/docs/stan-users-guide/regression.html) in the [Stan users guide](https://mc-stan.org/docs/stan-users-guide/index.html):

```{r}
#| class-output: stan
cat(readLines("lm.stan"), sep = "\n")
```

#### Stancode generated by calling `brms::brm`

The R add-on package *brms* [@Buerkner2017;@Buerkner2018] allows to implent advanced regression models without being an expert in 'Stan-programming'. 

Here is the Stan-code that is implemented by 'brms' for our linear regression model example:

```{r}
#| eval: false
#| echo: true
brms::make_stancode(brms::bf(y ~ x_1 + x_2, center = F), data = df)
```

```{r}
#| class-output: stan
cat(readLines("lm_brms.stan"), sep = "\n")
```

### Add-On Linear Model: B) Posterior predictive check: an introduction 'by hand'

Having an `lm` object already, it is rather straightforward to get posterior samples by using function `sim` from the *arm* [@GelmanSu2024] package:

```{r}
#| eval: true
#| echo: true
library("arm")
S <- sim(m)
str(S)
S <- cbind(S@coef, 'sigma' = S@sigma)
head(S)
```

Predict the response for the covariate data as provided by the original data-frame `df` - here only by using the first posterior sample:

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimH
#| fig-cap: "Histogram of the original and the posterior predicted response sample."
s <- 1
S[s, ]
mu_s <- S[s, '(Intercept)'] + S[s, 'x_1'] * df$x_1 + S[s, 'x_2'] * df$x_2
y_s <- rnorm(n = nrow(df), mean = mu_s, sd = S[s, 'sigma'])
pp <- rbind(data.frame(y = df$y, source = "original", s = s), 
            data.frame(y = y_s, source = "predicted", s = s))
ggplot(data = pp, aes(x = y, fill = source)) + 
  geom_histogram(alpha = .5, position = "identity")
```

Now let's repeat the same for 9 different posterior samples:

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimI
#| fig-cap: "Histogram of the original and the posterior predicted response sample."
pp <- NULL
for (s in 1:9) {
  mu_s <- S[s, '(Intercept)'] + S[s, 'x_1'] * df$x_1 + S[s, 'x_2'] * df$x_2
  y_s <- rnorm(n = nrow(df), mean = mu_s, sd = S[s, 'sigma'])
  pp <- rbind(pp, 
              data.frame(y = df$y, source = "original", s = s), 
              data.frame(y = y_s, source = "predicted", s = s))
}
ggplot(data = pp, aes(x = y, fill = source)) + 
  geom_histogram(alpha = .5, position = "identity") + 
  facet_wrap(~ s)
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimJ
#| fig-cap: "The same as in @fig-plotlmsimI, but now using kernel density visualisations."
ggplot(data = pp, aes(x = y, fill = source)) + 
  geom_density(alpha = .5, position = "identity") + 
  facet_wrap(~ s)
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimK
#| fig-cap: "The same as in @fig-plotlmsimI or @fig-plotlmsimJ, but now using empirical cumulative density function visualisations."
ggplot(data = pp, aes(x = y, colour = source)) + 
  stat_ecdf() + 
  facet_wrap(~ s)
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotlmsimL
#| fig-cap: "The same as in @fig-plotlmsimL, but now within one plotting window: This visualisation is what `brms::pp_check` will produce if applied on a `brm` object."
ggplot(data = subset(pp, source == "predicted"),
       aes(x = y, group = s)) + 
  geom_density(position = "identity", fill = NA, colour = "grey") +
  geom_density(data = subset(pp, source == "original" & s == 1), 
               aes(x = y), linewidth = 1)
```

`r if (knitr::is_latex_output()) '\\clearpage'`

# Binary Regression Model

```{r}
#| eval: true
#| echo: true
rm(list = ls())
library("ggplot2")
library("plyr")
```

## Data Simulation

Data are simulated similarly as for the linear model:

```{r}
#| eval: true
#| echo: true
set.seed(123)
N <- 500
df <- data.frame(x_1 = runif(n = N), 
                 x_2 = runif(n = N))
(beta_0 <- rnorm(n = 1, mean = 0, sd = .1))
(beta_x_1 <- rnorm(n = 1, mean = 1, sd = .1))
(beta_x_2 <- rnorm(n = 1, mean = -.5, sd = .1))
df$eta <- beta_0 + beta_x_1 * df$x_1 + beta_x_2 * df$x_2
df$y <- rbinom(n = N, size = 1, prob = plogis(q = df$eta))
```

### Visualisations

```{r}
#| eval: true
#| echo: true
#| label: fig-plotbinomsimA
#| fig-cap: "Scatterplot of covariate `x_1` with response `y` - each individual observation is coloured according to the second covariate `x_2`, and additionally 'jittered' in vertical direction."
df$x_1_c <- cut(df$x_1, breaks = seq(0, 1, by = .1), 
                include.lowest = T, 
                labels = seq(.05, .95, by = .1))
df$x_1_c <- as.numeric(as.character(df$x_1_c))
df_p_A <- ddply(df, c("x_1_c"), summarise, 
                p = mean(y > .5))
df_p_A <- data.frame('p' = rep(df_p_A$p, each = 2), 
                     'x_1' = sort(c(df_p_A$x_1_c - .05, 
                                    df_p_A$x_1_c + .05)))
df_p_B <- data.frame('x_1' = seq(0, 1, by = .01), 
                     'p' = plogis(beta_0 + 
                                    beta_x_1 * seq(0, 1, by = .01) + 
                                    beta_x_2 * .5))
set.seed(0)
ggplot(data = df, aes(x = x_1, y = y)) + 
  geom_jitter(aes(color = x_2), width = 0, height = .1) +
  geom_line(data = df_p_A, aes(y = p, group = p)) +
  geom_line(data = df_p_B, aes(y = p), linetype = 2)
## ... 'not as linear as it seems':
# plot(df_p_B$x_1[-1], diff(df_p_B$p))
```

## Modeling

The basic R command for (frequentist) estimation of the parameters of a binary regression model is a call to the function `glm` with `family` argument `binomial`:

```{r}
#| eval: true
#| echo: true
m <- glm(y ~ x_1 + x_2, data = df, 
         family = binomial(link = 'logit'))
summary(m)
```

### Visualisations

```{r}
#| eval: true
#| echo: true
#| label: fig-plotbinomsimB
#| fig-cap: "Scatterplot of covariate `x_1` with the true linear predictor `eta` - each individual observation is coloured according to the second covariate `x_2`. The line gives the point estimation for the conditional expectation with the second covariate `x_2` fixed to $0.5$."
nd <- data.frame('x_1' = 0:1, 'x_2' = .5)
(nd$eta <- predict(m, newdata = nd, type = 'link'))
coef(m)[1] + c(0, 1) * coef(m)[2] + .5 * coef(m)[3]
pA <- ggplot(data = df, aes(x = x_1, y = eta, color = x_2)) + 
  geom_point() + 
  geom_line(data = nd, aes(x = x_1, y = eta, color = x_2))
nd <- data.frame('x_1' = .5, 
                 'x_2' = 0:1)
(nd$eta <- predict(m, newdata = nd, type = 'link'))
coef(m)[1] + .5 * coef(m)[2] + c(0, 1) * coef(m)[3]
pB <- ggplot(data = df, aes(x = x_2, y = eta, color = x_1)) + 
  geom_point() + 
  geom_line(data = nd, aes(x = x_2, y = eta, color = x_1))
cowplot::plot_grid(pA, pB, ncol = 2)
```

```{r}
#| eval: true
#| echo: true
#| label: fig-plotbinomsimC
#| fig-cap: "Scatterplot of covariate `x_1` with the true conditional expectation `p` - each individual observation is coloured according to the second covariate `x_2`. The line gives the point estimation for the conditional expectation with the second covariate `x_2` fixed to $0.5$."
nd <- data.frame(x_1 = seq(0, 1, by = .1), 
                 x_2 = .5)
(nd$p <- predict(m, newdata = nd, type = 'response'))
plogis(coef(m)[1] + seq(0, 1, by = .1) * coef(m)[2] + 
         .5 * coef(m)[3])
ggplot(data = df, aes(x = x_1, y = y)) + 
  geom_jitter(aes(color = x_2), width = 0, height = .1) +
  geom_line(data = nd, aes(x = x_1, y = p, color = x_2)) +
  geom_line(data = df_p_B, aes(y = p), linetype = 2)
```

### Estimated Expected Value

We can apply the [Bernstein-von Mises theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem) to estimate the *expected value*:

- **Fit the model**: Obtain the maximum likelihood estimate for the model's coefficients (`coef`) along with their variance-covariance matrix (`vcov`).
- **Simulate coefficients**: Perform an 'informal' Bayesian posterior simulation using the multivariate normal distribution, based on the *Bernstein-von Mises theorem*.
- **Convert simulated coefficients**: Apply an appropriate transformation to the simulated coefficients to compute the *simulated quantity of interest*. This quantity typically depends on the values of all explanatory variables, and researchers may:
- Focus on a specific observation (usually an 'average'), or
- Average across all sample observations.

In both cases, the applied transformation incorporates the researcher’s specific choice.

```{r}
#| eval: true
#| echo: true
#| label: fig-plotbinomsimD
#| fig-cap: "Scatterplot of covariate `x_1` with the true conditional expectation `mu` - each individual observation is coloured according to the second covariate `x_2`. The line gives the point estimation for the conditional expectation with the second covariate `x_2` fixed to $0.5$."
library("MASS")
coef(m)
vcov(m)
set.seed(0)
B <- mvrnorm(n = 100, mu = coef(m), Sigma = vcov(m))
head(B)
nd <- expand.grid('x_1' = nd$x_1, 
                  'x_2' = nd$x_2, 
                  's' = 1:nrow(B))
head(nd)
nd$p <- plogis(B[nd$s, 1] + B[nd$s, 2] * nd$x_1 + 
                 B[nd$s, 3] * nd$x_2)
dd <- ddply(nd, c('x_1'), summarise,
            p_mean = mean(p),
            p_lwr_95 = quantile(p, prob = .025),
            p_upr_95 = quantile(p, prob = .975),
            p_lwr_9 = quantile(p, prob = .05),
            p_upr_9 = quantile(p, prob = .95),
            p_lwr_75 = quantile(p, prob = .125),
            p_upr_75 = quantile(p, prob = .875))
set.seed(0)
ggplot(data = df, aes(x = x_1)) + 
  geom_jitter(aes(y = y, color = x_2), width = 0, height = .1) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = p_lwr_95, 
                             ymax = p_upr_95), alpha = .4) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = p_lwr_9, 
                             ymax = p_upr_9), alpha = .4) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = p_lwr_75, 
                             ymax = p_upr_75), alpha = .4) +
  geom_line(data = dd, aes(y = p_mean))
```

`r if (knitr::is_latex_output()) '\\clearpage'`

# Poisson Regression Model

```{r}
#| eval: true
#| echo: true
rm(list = ls())
library("ggplot2")
```

## Data Simulation

Data are simulated similarly as for the linear model:

```{r}
#| eval: true
#| echo: true
set.seed(123)
N <- 500
df <- data.frame(x_1 = runif(n = N), 
                 x_2 = runif(n = N))
(beta_0 <- rnorm(n = 1, mean = 0, sd = .1))
(beta_x_1 <- rnorm(n = 1, mean = 1, sd = .1))
(beta_x_2 <- rnorm(n = 1, mean = -.5, sd = .1))
df$eta <- beta_0 + beta_x_1 * df$x_1 + beta_x_2 * df$x_2
df$y <- rpois(n = N, lambda = exp(df$eta))
```

### Visualisations

```{r}
#| eval: true
#| echo: true
#| label: fig-plotpoissimB
#| fig-cap: "Scatterplot of covariate `x_1` with response `y` - each individual observation is coloured according to the second covariate `x_2`."
df$x_1_c <- cut(df$x_1, breaks = seq(0, 1, by = .1), 
                include.lowest = T, 
                labels = seq(.05, .95, by = .1))
df$x_1_c <- as.numeric(as.character(df$x_1_c))
df_p_A <- ddply(df, c("x_1_c"), summarise, 
                mu = mean(y))
df_p_A <- data.frame('mu' = rep(df_p_A$mu, each = 2), 
                     'x_1' = sort(c(df_p_A$x_1_c - .05, 
                                    df_p_A$x_1_c + .05)))
df_p_B <- data.frame('x_1' = seq(0, 1, by = .01), 
                     'mu' = exp(beta_0 + 
                                  beta_x_1 * seq(0, 1, by = .01) + 
                                  beta_x_2 * .5))
set.seed(0)
ggplot(data = df, aes(x = x_1, y = y)) + 
  geom_jitter(aes(color = x_2), width = 0, height = .1) +
  geom_line(data = df_p_A, aes(y = mu, group = mu)) +
  geom_line(data = df_p_B, aes(y = mu), linetype = 2)
```

## Modeling

The basic R command for (frequentist) estimation of the parameters of a binary regression model is a call to the function `glm` with `family` argument `poisson(link = 'log')`:

```{r}
#| eval: true
#| echo: true
m <- glm(y ~ x_1 + x_2, data = df, family = poisson(link = 'log'))
summary(m)
```

### Estimated Expected Value

Let's again apply the [Bernstein-von Mises theorem](:https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem)

```{r}
#| eval: true
#| echo: true
#| label: fig-plotpoissimC
#| fig-cap: "Scatterplot of covariate `x_1` with the response observations `y` - each individual observation is coloured according to the second covariate `x_2`. The line gives the point estimation for the conditional expectation with the second covariate `x_2` fixed to $0.5$, the coloured intervals give point-wise central 75%, 90%, and 95% credible intervals for the conditional expectation."
library("MASS")
coef(m)
vcov(m)
set.seed(0)
B <- mvrnorm(n = 100, mu = coef(m), Sigma = vcov(m))
head(B)
nd <- expand.grid('x_1' = seq(0, 1, by = .1), 
                  'x_2' = .5, 
                  's' = 1:nrow(B))
head(nd)
nd$mu <- exp(B[nd$s, 1] + 
               B[nd$s, 2] * nd$x_1 + 
               B[nd$s, 3] * nd$x_2)
dd <- ddply(nd, c('x_1'), summarise,
            mu_mean = mean(mu),
            mu_lwr_95 = quantile(mu, prob = .025),
            mu_upr_95 = quantile(mu, prob = .975),
            mu_lwr_9 = quantile(mu, prob = .05),
            mu_upr_9 = quantile(mu, prob = .95),
            mu_lwr_75 = quantile(mu, prob = .125),
            mu_upr_75 = quantile(mu, prob = .875))
df_p_B <- data.frame('x_1' = seq(0, 1, by = .01), 
                     'mu' = exp(coef(m)[1] + 
                                  coef(m)[2] * seq(0, 1, by = .01) + 
                                  coef(m)[3] * .5))
set.seed(0)
ggplot(data = df, aes(x = x_1)) + 
  geom_jitter(aes(y = y, color = x_2), width = 0, height = .1) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = mu_lwr_95, 
                             ymax = mu_upr_95), alpha = .4) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = mu_lwr_9, 
                             ymax = mu_upr_9), alpha = .4) +
  geom_ribbon(data = dd, aes(x = x_1, ymin = mu_lwr_75, 
                             ymax = mu_upr_75), alpha = .4) +
  geom_line(data = dd, aes(y = mu_mean)) +
  geom_line(data = df_p_B, aes(y = mu), linetype = 2)
```

`r if (knitr::is_latex_output()) '\\clearpage'`

# Mixed models 

... a.k.a. *hierarchical model*, *multilevel model*, ...

```{r}
#| eval: true
#| echo: true
rm(list = ls())
library("lme4")
library("ggplot2")
library("plyr")
```

## Data Simulation Function `f_sim_data`

```{r}
#| eval: true
#| echo: true
f_sim_data <- function(seed, type) {
  set.seed(seed) # Set seed for reproducibility
  parameters <- list(## Global intercept: 
    "beta_0" = rnorm(n = 1, mean = 2, sd = .1),
    ## Global slope of 'x':
    "beta_x" = rnorm(n = 1, mean = 1.5, sd = .1),
    ## Standard deviation of residuals:
    "sigma" = abs(rnorm(n = 1, mean = 1, 
                        sd = .1)))
  if (type == "Random_Intercept") {
    ## Standard deviation of random intercept parameters:
    parameters$'sigma_u' <- abs(rnorm(n = 1, mean = 1, sd = .1))
    ## Number of groups: 
    parameters$'G' <- 30
    ## Number of observations per group:
    parameters$'n_per_g' <- 30
    g <- rep(1:parameters$'G', each = parameters$'n_per_g')
    x <- runif(n = parameters$'G' * parameters$'n_per_g', 
               min = -1, max = 1)
    df <- data.frame('x' = x, 
                     'g' = g)
    df$u <- rnorm(n = parameters$'G', mean = 0, 
                  sd = parameters$'sigma_u')[df$g]
    df$mu <- parameters$'beta_0' + 
      parameters$'beta_x' * df$x + df$u
    attributes(df)$'type' <- type 
    attributes(df)$'parameters' <- parameters
  }
  if (type == "Nested") {
    ## Standard deviation of random intercept parameters:
    parameters$'sigma_u_a' <- abs(rnorm(n = 1, mean = 1, sd = .1))
    parameters$'sigma_u_b' <- abs(rnorm(n = 1, mean = 1, sd = .1))
    ## Number of groups in 1st level:
    parameters$'G_a' <- 30
    ## Number of observations per group:
    parameters$'n_per_g_a' <- 30
    ## Number of groups in 2nd level:
    parameters$'G_b' <- 10
    ## Number of observations per group:
    parameters$'n_per_g_b' <- 6
    gr <- as.data.frame(expand.grid('g_a' = 1:parameters$'G_a', 
                                    'g_b' = 1:parameters$'G_b'))
    df <- gr[rep(1:nrow(gr), each = parameters$'n_per_g_b'), ]
    df <- df[order(df$g_a, df$g_b), ]
    rownames(df) <- NULL
    df$g_ab <- paste0(df$g_a, "_", df$g_b)
    df$x <- runif(n = parameters$'G_a' * parameters$'n_per_g_a', 
                  min = -1, max = 1)
    u_a <- rnorm(n = parameters$'G_a', mean = 0, 
                 sd = parameters$'sigma_u_a')
    df$u_a <- u_a[df$g_a]
    u_b <- rnorm(n = length(unique(df$g_ab)), mean = 0, 
                 sd = parameters$'sigma_u_b')
    names(u_b) <- unique(df$g_ab)
    df$u_b <- as.numeric(u_b[df$g_ab])
    df$mu <- parameters$'beta_0' + parameters$'beta_x' * df$x + 
      df$u_a + df$u_b
    attributes(df)$'type' <- type 
    attributes(df)$'parameters' <- parameters
  }
  epsilon <- rnorm(n = nrow(df), mean = 0, sd = parameters$'sigma')
  df$y <- df$mu + epsilon
  return(df)
}
```

## Random Intercept Model

```{r}
#| eval: true
#| echo: true
df <- f_sim_data(seed = 0, type = "Random_Intercept")
head(df)
unlist(attributes(df)$parameters)
m <- lmer(y ~ x + (1 | g), data = df)
summary(m)
```

### ... small simulation study

```{r}
#| eval: false
#| echo: true
R <- 50
ci_df <- NULL
for (r in 1:R) {
  ## Simulate data:
  df <- f_sim_data(seed = r, type = "Random_Intercept")
  ## Estimate models:
  lm_model <- lm(y ~ x, data = df)
  lmer_model <- lmer(y ~ x + (1 | g), data = df)
  ## Extract confidence intervals:
  lm_ci <- confint(lm_model, level = 0.95)
  lmer_ci <- suppressMessages(confint(lmer_model, level = 0.95))
  ## Store results:
  par_name <- "sigma"
  tmp <- data.frame(r = r,
                    par_name = par_name,
                    Value = rep(attributes(df)$parameters$sigma, 
                                times = 2),
                    Model = c("lm", "lmer"),
                    Estimate = c(summary(lm_model)$sigma, 
                                 summary(lmer_model)$sigma),
                    CI_Low = rep(NA, 2),
                    CI_High = c(NA, 2))
  ci_df <- rbind(ci_df, tmp)
  par_name <- "x"
  tmp <- data.frame(r = r,
                    par_name = par_name,
                    Value = rep(attributes(df)$parameters$beta_x, 
                                times = 2),
                    Model = c("lm", "lmer"),
                    Estimate = c(coef(lm_model)[par_name], 
                                 fixef(lmer_model)[par_name]),
                    CI_Low = c(lm_ci[par_name, 1], 
                               lmer_ci[par_name, 1]),
                    CI_High = c(lm_ci[par_name, 2], 
                                lmer_ci[par_name, 2]))
  ci_df <- rbind(ci_df, tmp)
  par_name <- "(Intercept)"
  tmp <- data.frame(r = r,
                    par_name = par_name,
                    Value = rep(attributes(df)$parameters$beta_0, 
                                times = 2),
                    Model = c("lm", "lmer"),
                    Estimate = c(coef(lm_model)[par_name], 
                                 fixef(lmer_model)[par_name]),
                    CI_Low = c(lm_ci[par_name, 1], 
                               lmer_ci[par_name, 1]),
                    CI_High = c(lm_ci[par_name, 2], 
                                lmer_ci[par_name, 2]))
  ci_df <- rbind(ci_df, tmp)
  cat(".")
}
ci_df$par_name <- factor(ci_df$par_name, 
                         levels = c("(Intercept)", "x", "sigma"))
```

```{r}
#| eval: true
#| echo: false
# save(ci_df, file = "mixed_models_ci_df.RData")
load(file = "mixed_models_ci_df.RData")
```

### ... small simulation study

```{r}
#| eval: true
#| echo: true
#| label: fig-mixedmodelA
#| fig-cap: "Simulation study results: Red dots show true underlying values."
ggplot(ci_df, aes(x = r)) +
  geom_pointrange(aes(y = Estimate, ymin = CI_Low, 
                      ymax = CI_High)) +
  geom_point(aes(y = Value), color = 2) +
  labs(y = "Parameter estimate & interval",
       x = "Simulation run") +
  facet_grid(cols = vars(Model), rows = vars(par_name), 
             scales = "free") +
  theme(legend.position = "none")
```

## Random Intercept with Random Slope Model

```{r}
#| eval: true
#| echo: true
#| label: fig-mixedmodelB
#| fig-cap: "Scatterplot for simulated data with random intercept und randon slope: Dashed lines shows the underlying group specific conditional expectation."
f_add_random_slope <- function(df, x_lab, g_lab) {
  ## assign(paste0("sigma_u_", x_label, "_", g_label), 1)
  sigma_u_slope <- abs(rnorm(n = 1, mean = 1, sd = .1))
  u_slope <- rnorm(length(unique(df[, g_lab])), mean = 0, 
                   sd = sigma_u_slope)
  df$u_slope <- u_slope[df[, g_lab]]
  df$y <- df$y + df[, x_lab] * df$u_slope
  attributes(df)$parameters[[paste0("sigma_u_", x_lab, "_", g_lab)]] <- 
    sigma_u_slope
  return(df)
}
df <- f_sim_data(seed = 0, type = "Random_Intercept")
df <- f_add_random_slope(df = df, x_lab = "x", g_lab = "g")
head(df)
gr <- expand.grid('x' = c(-1, 1),
                  'g' = 1:attributes(df)$parameters$G)
dd <- ddply(df, c("g"), summarise,
            'intercept' = u[1],
            'slope' = u_slope[1])
gr$y <- attributes(df)$parameters$beta_0 + dd$intercept[gr$g] + 
  gr$x * (attributes(df)$parameters$beta_x + dd$slope[gr$g])
ggplot(data = df, aes(x = x, y = y)) + 
  geom_line(data = data.frame(x = c(-1, 1), 
                              y = attributes(df)$parameters$beta_0 + 
                                c(-1, 1) * 
                                attributes(df)$parameters$beta_x)) + 
  geom_point(alpha = .5) + 
  geom_line(data = gr, aes(group = g), linetype = 2) +
  facet_wrap(~ g)
unlist(attributes(df)$parameters)
m <- lmer(y ~ x + (1 + x|g), data = df)
summary(m)
```

## Nested Model

```{r}
#| eval: true
#| echo: true
#| label: fig-mixedmodelC
#| fig-cap: "Visual check of equality of coefficient values."
df <- f_sim_data(seed = 0, type = "Nested")
head(df)
## ... two alternatives:
m1 <- lmer(y ~ x + (1|g_a/g_b), data = df)
m2 <- lmer(y ~ x + (1|g_a) + (1|g_ab), data = df)
unlist(attributes(df)$parameters)
summary(m1)
summary(m2)
cowplot::plot_grid(
  ggplot(data = data.frame(x = ranef(m1)$'g_a'[, 1], 
                           y = ranef(m2)$'g_a'[, 1])) + 
    geom_point(aes(x = x, y = y)) + 
    geom_abline(intercept = 0, slope = 1) + 
    labs(x = "ranef(m1)$'g_a'[, 1]", y = "ranef(m2)$'g_a'[, 1]"), 
  ggplot(data = data.frame(x = sort(ranef(m1)$'g_b:g_a'[, 1]), 
                           y = sort(ranef(m2)$'g_ab'[, 1]))) + 
    geom_point(aes(x = x, y = y)) + 
    geom_abline(intercept = 0, slope = 1) + 
    labs(x = "sort(ranef(m1)$'g_b:g_a'[, 1])", 
         y = "sort(ranef(m2)$'g_ab'[, 1])"))
```

### ... add covariate 'z' as constant within 2nd level

```{r}
#| eval: true
#| echo: true
#| label: fig-mixedmodelD
#| fig-cap: "Scatterplot of two-level grouped data with constant covariate for 2nd level."
f_add_covariate_constant_within_b <- function(df) {
  attributes(df)$'parameters'$'beta_z' <- rnorm(n = 1, mean = 1.5, 
                                                sd = .1)
  if (attributes(df)$type != "Nested") {
    stop("Use type 'Nested' to generate 'df'.")
  }
  z <- runif(n = length(unique(df$g_ab)), min = -1, max = 1)
  names(z) <- unique(df$g_ab)
  df$z <- as.numeric(z[df$g_ab])
  df$y <- df$y + df$z * attributes(df)$'parameters'$'beta_z'
  return(df)
}
df <- f_sim_data(seed = 0, type = "Nested")
df <- f_add_covariate_constant_within_b(df = df)
ggplot(data = df, aes(x = x, y = y, colour = z)) + 
  geom_point() + 
  facet_wrap(~ g_a) + 
  theme(legend.position = 'top')
m <- lmer(y ~ x + z + (1 | g_a / g_b), data = df)
summary(m)
```

`r if (knitr::is_latex_output()) '\\clearpage'`

# References {-}

::: {#refs}
:::