<!DOCTYPE html>
<html>
<head>
<title>Probability and Probabilistic Modeling</title>
<script type="text/x-mathjax-config">
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ["\\(", "\\)"]],
      processEscapes: true,
    }
  }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
.math {
display: block;
margin: 1em 0;
}
</style>
</head>
<body>
<div style="max-width:500px; word-wrap:break-word;">
<h1>Basics of Probabilistic Modeling</h1>
<p>
<a href="https://uncertaintree.github.io/oer/basic_probability/probability_interpretations.html">Interpretations of Probability</a>
</p>

<h3>Mathematical definiton</h3>
<p>
At its core, probability is a mathematical concept that's based on the idea of assigning a numerical value to the likelihood of an event occurring. 
	This numerical value, known as the probability, is typically expressed as a number between \(0\) and \(1\), where \(0\) represents an impossible event and \(1\) represents a certain event. 
</p>
<p>
As an example suppose we have an event denoted $A$ and we know that whatever that outcome can be that is not included in - or equal to - \(A\), is equally likely to be seen than \(A\) itself, than this knowledge is quantifiable as:
</p>	
<div class="math">
$$P(A) = \frac{1}{2}$$
</div>

<h2>Probability Mass and Density</h2>

Probability can be represented in two ways: as a probability mass or as a probability density. 
	A probability mass is a number between \(0\) and \(1\) that assigns a probability to each possible outcome of a discrete random variable.

<div class="math">
$$P(X = x) = \frac{1}{2}$$
</div>

A probability density for a random variable \(X\) is a function where for each value \(x\) of \(X\) - visualized on the x-axis -, the function value on the y-axis shows the relative likelihood of observing a value "close" to \(x\). 
In the "interval perspective", a probability density is a function that assigns a probability - a number between 0 and 1 - to an interval with positive length in the domain of a continuous random variable.

<div class="math">
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
</div>

For example, let's say we're interested in the probability of a tree growing to a certain height. 
We can represent this as a probability density distribution, where the probability of a tree growing to a height between 8 and 12 meters is represented by a continuous function, and the integral of this function in this interval is the probability mass for $A=$'a tree has a height between 8 and 12 meters'

So we note that probability mass and density are not mutually exclusive concepts: 
	whenever we integrate the density on some interval, we get to a probability mass. 

For example, a discrete probability distribution can be represented as a probability mass distribution, while a continuous probability distribution can be represented as a probability density distribution.
	
<h2>Expectation</h2>

Expectation is a fundamental concept in probability theory that represents the average value of a random variable. 
	It's a way of quantifying the expected value of a variable, and it's a crucial concept in probabilistic modeling.

<div class="math">
$$E(X) = \int_{-\infty}^{\infty}xf(x)dx$$
</div>

<h2>Variance and Standard Deviation</h2>
	<p>Variance and standard deviation are two related concepts that represent the spread of a probability distribution. Variance is a measure of the average squared difference between a variable and its expected value, while standard deviation is the square root of the variance.</p>
	<div class="math">
		$$\text{Var}(X) = E[(X-E(X))^2]$$
	</div>
	<h2>Likelihood</h2>
	<p>Likelihood is a fundamental concept in probability theory that represents the probability of observing a particular set of data given a particular model. It's a way of quantifying the probability of a model given the data, and it's a crucial concept in statistical modeling.</p>
	<div class="math">
		$$L(\theta|x) = P(X=x|\theta)$$
	</div>
	<h2>Probabilistic Modeling</h2>
	<p>Probabilistic modeling is a statistical framework that allows us to model complex systems by representing the uncertainty associated with the system's behavior. It's a way of quantifying the uncertainty associated with the system's behavior and making predictions about future outcomes.</p>
	<div class="math">
		$$P(X=x|\theta) = \frac{1}{Z(\theta)}\exp\left(\theta^Tx\right)$$
	</div>
	<h2>Generating Artificial Data with R</h2>
	<p>Now that we have a probabilistic model in mind, let's see how we can generate artificial data using R. We'll use the `rnorm()` function to generate random values for tree diameter and height, and then use the `lm()` function to fit a linear regression model to the data.</p>
	<div class="math">
		```R
		# Set the seed for reproducibility
		set.seed(123)

		# Generate random values for tree diameter and height
		D <- rnorm(100, mean = 20, sd = 5)
		H <- rlnorm(100, meanlog = 10, sdlog = 2)

		# Fit a linear regression model to the data
		model <- lm(H ~ D)
		summary(model)
		```
	</div>
	<h2>Conclusion</h2>
	<p>In this chapter, we've covered the fundamentals of probability, including probability mass and density, expectation, variance and standard deviation, and likelihood. We've also seen how to generate artificial data using R and how to use a probabilistic model to model complex relationships between variables.</p>
	<p>Probability is a powerful tool that allows us to quantify the uncertainty associated with random events. By using probability distributions to represent the uncertainty in complex systems, we can make predictions about future outcomes and explore the underlying mechanisms driving the system.</p>
	<h2>References</h2>
	<p>If you're interested in learning more about probability and probabilistic modeling, here are some references that you might find helpful:</p>
	<ul>
		<li>"Probability and Statistics for Engineers and Scientists" by Ronald E. Walpole, Raymond H. Myers, Sharon L. Myers, and Keying E. Ye</li>
		<li>"Introduction to Probability and Statistics for Engineers and Scientists" by Sheldon M. Ross</li>
		<li>"Probability Theory: The Logic of Science" by E.T. Jaynes</li>
		<li>"Bayesian Data Analysis" by Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin</li>
		<li>"Markov Chain Monte Carlo Methods and Applications" by Wally R. Gilks, Sylvia Richardson, and David J. Spiegelhalter</li>
	</ul>
</body>
</html>
